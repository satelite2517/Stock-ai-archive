{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최종 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 코멘트 한문장씩 파싱\n",
    "import ast\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from transformers import pipeline, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def expand_comments_post(df, stock_name):\n",
    "    expanded_comments = []\n",
    "    for comment in df['Comments']:\n",
    "        # 문자열의 앞뒤 대괄호 제거\n",
    "        comment = comment.strip('[]')\n",
    "        \n",
    "        expanded_comment = ast.literal_eval(comment)\n",
    "        for c in expanded_comment:\n",
    "            sentences = c.split('. ')\n",
    "            expanded_comments.extend(sentences)\n",
    "    \n",
    "    repeated_indices = df.index.repeat(df['Comments'].apply(lambda x: sum(len(c.split('. ')) for c in ast.literal_eval(x.strip('[]')))))\n",
    "    \n",
    "    expanded_df = pd.DataFrame({\n",
    "        'date': pd.to_datetime(df.loc[repeated_indices, 'Created Time']).reset_index(drop=True),\n",
    "        'stock': stock_name,\n",
    "        'comment': expanded_comments\n",
    "    })\n",
    "    \n",
    "    return expanded_df\n",
    "\n",
    "# 레딧 전체 함수\n",
    "def reddit_posts(stock_name): \n",
    "    file_path = f'/Users/jiheelee/Desktop/2024-1/Stock-info-archive/data/reddit_post_name/reddit_posts_{stock_name}.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    expanded_data = expand_comments_post(data, stock_name)\n",
    "    \n",
    "    comments = expanded_data['comment'].tolist()\n",
    "    unique_comments = list(OrderedDict.fromkeys(comments))\n",
    "    final_data = expanded_data.drop_duplicates(subset=['comment'])\n",
    "    \n",
    "    # 모델과 토크나이저 로드\n",
    "    model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # TensorFlow 기반의 감정 분석 파이프라인 생성\n",
    "    sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework='tf')\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    scores = []\n",
    "    sent = []\n",
    "\n",
    "    # comments 리스트를 250개씩 나누어 함수 실행 후 결과 저장\n",
    "    batch_size = 5\n",
    "    \n",
    "    texts = list(OrderedDict.fromkeys(comments))\n",
    "    for i in range(0, len(comments), batch_size):\n",
    "        batch = unique_comments[i:i + batch_size]\n",
    "        results = sentiment_pipeline(batch)\n",
    "        for text, result in zip(texts, results):\n",
    "            scores.append(result['score'])\n",
    "            sent.append(result['label'])\n",
    "            # print(f\"Text: {text}\\nSentiment: {result['label']}, Score: {result['score']}\\n\")\n",
    "    \n",
    "    result = pd.DataFrame({'date': final_data['date'], 'stock': stock_name, 'sent': sent, 'score': scores})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def expand_comments_key(df, stock_name):\n",
    "    expanded_comments = []\n",
    "    for comment in df['Comments']:\n",
    "        # 문자열의 앞뒤 대괄호 제거\n",
    "        comment = comment.strip('[]')\n",
    "        \n",
    "        expanded_comment = ast.literal_eval(comment)\n",
    "        for c in expanded_comment:\n",
    "            sentences = c.split('. ')\n",
    "            expanded_comments.extend(sentences)\n",
    "    \n",
    "    repeated_indices = df.index.repeat(df['Comments'].apply(lambda x: sum(len(c.split('. ')) for c in ast.literal_eval(x.strip('[]')))))\n",
    "    \n",
    "    expanded_df = pd.DataFrame({\n",
    "        'date': pd.to_datetime(df.loc[repeated_indices, 'Created Time']).reset_index(drop=True),\n",
    "        'stock': stock_name,\n",
    "        'query': df.loc[repeated_indices, 'Query'].reset_index(drop=True),\n",
    "        'comment': expanded_comments\n",
    "    })\n",
    "    \n",
    "    return expanded_df\n",
    "\n",
    "\n",
    "def reddit_keys(stock_name): \n",
    "    file_path = f'/Users/jiheelee/Desktop/2024-1/Stock-info-archive/data/reddit_post_keyword/reddit_posts_{stock_name}.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    expanded_data = expand_comments_key(data, stock_name)\n",
    "    expanded_data['comment'] = expanded_data['comment'].apply(lambda x: x[:350] if len(x) > 350 else x)\n",
    "    \n",
    "    comments = expanded_data['comment'].tolist()\n",
    "    unique_comments = list(OrderedDict.fromkeys(comments))\n",
    "    final_data = expanded_data.drop_duplicates(subset=['comment'])\n",
    "    \n",
    "    # 모델과 토크나이저 로드\n",
    "    model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # TensorFlow 기반의 감정 분석 파이프라인 생성\n",
    "    sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, framework='tf')\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    scores = []\n",
    "    sent = []\n",
    "\n",
    "    # comments 리스트를 250개씩 나누어 함수 실행 후 결과 저장\n",
    "    batch_size = 5\n",
    "    \n",
    "    texts = list(OrderedDict.fromkeys(comments))\n",
    "    for i in range(0, len(comments), batch_size):\n",
    "        batch = unique_comments[i:i + batch_size]\n",
    "        results = sentiment_pipeline(batch)\n",
    "        for text, result in zip(texts, results):\n",
    "            scores.append(result['score'])\n",
    "            sent.append(result['label'])\n",
    "            # print(f\"Text: {text}\\nSentiment: {result['label']}, Score: {result['score']}\\n\")\n",
    "    \n",
    "    result = pd.DataFrame({'date': final_data['date'], 'stock': stock_name, 'sent': sent, 'score': scores})\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# reddit_posts_AAPL = reddit_posts('AAPL')\n",
    "# reddit_posts_GOOGL = reddit_posts('GOOGL')\n",
    "# reddit_posts_AMZN = reddit_posts('AMZN')\n",
    "# reddit_posts_META = reddit_posts('META')\n",
    "# reddit_posts_MSFT = reddit_posts('MSFT')\n",
    "# reddit_posts_NVDA = reddit_posts('NVDA')\n",
    "# reddit_posts_TSLA = reddit_posts('TSLA')\n",
    "# reddit_keys_AAPL = reddit_keys('AAPL')\n",
    "# reddit_keys_GOOGL = reddit_keys('GOOGL')\n",
    "# reddit_keys_AMZN = reddit_keys('AMZN')\n",
    "# reddit_keys_META = reddit_keys('META')\n",
    "# reddit_keys_MSFT = reddit_keys('MSFT')\n",
    "# reddit_keys_NVDA = reddit_keys('NVDA')\n",
    "# reddit_keys_TSLA = reddit_keys('TSLA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_posts_AAPL.to_csv('result/reddit_post_AAPL_result.csv')\n",
    "# reddit_posts_AMZN.to_csv('result/reddit_post_AMZN_result.csv')\n",
    "# reddit_posts_GOOGL.to_csv('result/reddit_post_GOOGL_result.csv')\n",
    "# reddit_posts_META.to_csv('result/reddit_post_META_result.csv')\n",
    "# reddit_posts_MSFT.to_csv('result/reddit_post_MSFT_result.csv')\n",
    "# reddit_posts_NVDA.to_csv('result/reddit_post_NVDA_result.csv')\n",
    "# reddit_posts_TSLA.to_csv('result/reddit_post_TSLA_result.csv')\n",
    "# reddit_keys_AAPL.to_csv('result/reddit_key_AAPL_result.csv')\n",
    "# reddit_keys_AMZN.to_csv('result/reddit_key_AMZN_result.csv')\n",
    "# reddit_keys_GOOGL.to_csv('result/reddit_key_GOOGL_result.csv')\n",
    "# reddit_keys_META.to_csv('result/reddit_key_META_result.csv')\n",
    "# reddit_keys_MSFT.to_csv('result/reddit_key_MSFT_result.csv')\n",
    "# reddit_keys_NVDA.to_csv('result/reddit_key_NVDA_result.csv')\n",
    "# reddit_keys_TSLA.to_csv('result/reddit_key_TSLA_result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_ma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
